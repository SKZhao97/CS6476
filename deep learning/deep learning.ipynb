{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import student_code as sc\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "data_path = osp.join('../data', '15SceneData')\n",
    "num_classes = 15\n",
    "\n",
    "# If you have a good Nvidia GPU with an appropriate environment, \n",
    "# try setting the use_GPU flag to True (the environment provided does\n",
    "# not support GPUs and we will not provide any support for GPU\n",
    "# computation in this project). Please note that \n",
    "# we will evaluate your implementations only using CPU mode so even if\n",
    "# you use a GPU, make sure your code runs in the CPU mode with the\n",
    "# environment we provided. \n",
    "use_GPU = False\n",
    "if use_GPU:\n",
    "    from utils_gpu import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has two main parts. In Part 1, you will train a deep network from scratch. In Part 2, you will \"fine-tune\" a trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Warm up! Training a Deep Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to code anything for this part. You will simply run the code we provided, but we want you to report the result you got. This section will also familiarize you with the steps of training a deep network from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "input_size = (64, 64)\n",
    "RGB = False  \n",
    "base_lr = 1e-2  # may try a smaller lr if not using batch norm\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create our datasets, by calling the create_datasets function from student_code. This function returns a separate dataset loader for each split of the dataset (training and testing/validation). Each dataloader is used to load the datasets after appling some pre-processing transforms. In Part 1, you will be asked to add a few more pre-processing transforms to the dataloaders by modifying this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 30\n",
      "Batch 20 / 30\n",
      "Done, mean = \n",
      "[0.45611589 0.45611589 0.45611589]\n",
      "std = \n",
      "[0.24786406 0.24786406 0.24786406]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 60\n",
      "Batch 20 / 60\n",
      "Batch 40 / 60\n",
      "Done, mean = \n",
      "[0.45549639 0.45549639 0.45549639]\n",
      "std = \n",
      "[0.24698076 0.24698076 0.24698076]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our network model using the SimpleNet class from student_code. The implementation provided in the SimpleNet class gives you a basic network. In Part 1, you will be asked to add a few more layers to this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 5, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(5, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Dropout(p=0.5)\n",
      "  )\n",
      "  (classifier): Conv2d(10, 15, kernel_size=(8, 8), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the network model.\n",
    "model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the loss function.\n",
    "# see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer and a learning rate scheduler\n",
    "optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# Currently a simple step scheduler.\n",
    "# See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "# and how to use them\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to train our network! We will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Experiment: part1\n",
      "n_epochs: 100\n",
      "do_val: True\n",
      "print_freq: 100\n",
      "num_workers: 4\n",
      "shuffle: True\n",
      "val_freq: 1\n",
      "checkpoint_file: None\n",
      "batch_size: 50\n",
      "resume_optim: True\n",
      "experiment: part1\n",
      "---------------------------------------\n",
      "part1 Epoch 0 / 100\n",
      "train part1: batch 0/29, loss 2.786, top-1 accuracy 6.000, top-5 accuracy 38.000\n",
      "train part1: loss 2.623312\n",
      "val part1: batch 0/59, loss 1.782, top-1 accuracy 42.000, top-5 accuracy 84.000\n",
      "val part1: loss 2.524197\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 1 / 100\n",
      "train part1: batch 0/29, loss 2.650, top-1 accuracy 22.000, top-5 accuracy 46.000\n",
      "train part1: loss 2.399990\n",
      "val part1: batch 0/59, loss 2.722, top-1 accuracy 18.000, top-5 accuracy 50.000\n",
      "val part1: loss 2.266163\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 2 / 100\n",
      "train part1: batch 0/29, loss 2.062, top-1 accuracy 40.000, top-5 accuracy 76.000\n",
      "train part1: loss 2.207893\n",
      "val part1: batch 0/59, loss 1.998, top-1 accuracy 38.000, top-5 accuracy 74.000\n",
      "val part1: loss 2.078472\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 3 / 100\n",
      "train part1: batch 0/29, loss 1.984, top-1 accuracy 40.000, top-5 accuracy 78.000\n",
      "train part1: loss 2.074508\n",
      "val part1: batch 0/59, loss 3.069, top-1 accuracy 8.000, top-5 accuracy 38.000\n",
      "val part1: loss 2.048696\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 4 / 100\n",
      "train part1: batch 0/29, loss 1.948, top-1 accuracy 40.000, top-5 accuracy 84.000\n",
      "train part1: loss 1.982009\n",
      "val part1: batch 0/59, loss 2.681, top-1 accuracy 8.000, top-5 accuracy 64.000\n",
      "val part1: loss 1.920544\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 5 / 100\n",
      "train part1: batch 0/29, loss 1.776, top-1 accuracy 40.000, top-5 accuracy 86.000\n",
      "train part1: loss 1.875918\n",
      "val part1: batch 0/59, loss 1.960, top-1 accuracy 36.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.842882\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 6 / 100\n",
      "train part1: batch 0/29, loss 1.430, top-1 accuracy 60.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.766218\n",
      "val part1: batch 0/59, loss 2.360, top-1 accuracy 10.000, top-5 accuracy 66.000\n",
      "val part1: loss 1.772619\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 7 / 100\n",
      "train part1: batch 0/29, loss 1.846, top-1 accuracy 34.000, top-5 accuracy 80.000\n",
      "train part1: loss 1.740773\n",
      "val part1: batch 0/59, loss 2.263, top-1 accuracy 28.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.773236\n",
      "Checkpoint saved\n",
      "part1 Epoch 8 / 100\n",
      "train part1: batch 0/29, loss 1.348, top-1 accuracy 54.000, top-5 accuracy 90.000\n",
      "train part1: loss 1.718312\n",
      "val part1: batch 0/59, loss 2.228, top-1 accuracy 26.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.828403\n",
      "Checkpoint saved\n",
      "part1 Epoch 9 / 100\n",
      "train part1: batch 0/29, loss 1.730, top-1 accuracy 50.000, top-5 accuracy 82.000\n",
      "train part1: loss 1.706409\n",
      "val part1: batch 0/59, loss 1.735, top-1 accuracy 42.000, top-5 accuracy 88.000\n",
      "val part1: loss 1.777186\n",
      "Checkpoint saved\n",
      "part1 Epoch 10 / 100\n",
      "train part1: batch 0/29, loss 1.783, top-1 accuracy 44.000, top-5 accuracy 84.000\n",
      "train part1: loss 1.609640\n",
      "val part1: batch 0/59, loss 2.269, top-1 accuracy 18.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.668497\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 11 / 100\n",
      "train part1: batch 0/29, loss 1.618, top-1 accuracy 54.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.578972\n",
      "val part1: batch 0/59, loss 1.927, top-1 accuracy 26.000, top-5 accuracy 86.000\n",
      "val part1: loss 1.638930\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 12 / 100\n",
      "train part1: batch 0/29, loss 1.529, top-1 accuracy 56.000, top-5 accuracy 84.000\n",
      "train part1: loss 1.555245\n",
      "val part1: batch 0/59, loss 1.713, top-1 accuracy 46.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.683953\n",
      "Checkpoint saved\n",
      "part1 Epoch 13 / 100\n",
      "train part1: batch 0/29, loss 1.527, top-1 accuracy 54.000, top-5 accuracy 88.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-104-44afdd0e76c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'n_epochs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'batch_size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'experiment'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'part1'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbest_prec1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best top-1 Accuracy = {:4.3f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_prec1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\研一\\CS6476 Computer Vision\\proj6\\code\\utils.py\u001b[0m in \u001b[0;36mtrain_val\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m       \u001b[1;31m# TRAIN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop1_prec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop5_prec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m       self.vis.line(X=np.asarray([epoch]), Y=np.asarray([loss]),\n\u001b[0;32m    236\u001b[0m         win=self.loss_win, name='train_loss', update='append', env=self.vis_env)\n",
      "\u001b[1;32mE:\\研一\\CS6476 Computer Vision\\proj6\\code\\utils.py\u001b[0m in \u001b[0;36mstep_func\u001b[1;34m(self, train)\u001b[0m\n\u001b[0;32m    198\u001b[0m       kwargs = dict(target=target, loss_fn=self.loss_fn,\n\u001b[0;32m    199\u001b[0m         optim=self.optimizer, train=train)\n\u001b[1;32m--> 200\u001b[1;33m       \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_feedfwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m       \u001b[1;31m# measure accuracy and calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\研一\\CS6476 Computer Vision\\proj6\\code\\utils.py\u001b[0m in \u001b[0;36mstep_feedfwd\u001b[1;34m(data, model, target, loss_fn, optim, train)\u001b[0m\n\u001b[0;32m    280\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0mdata_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m   \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\研一\\CS6476 Computer Vision\\proj6\\code\\student_code.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;33m-\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m     \"\"\"\n\u001b[1;32m--> 225\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python\\envs\\cs6476p6\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mthreshold\u001b[1;34m(input, threshold, value, inplace)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \"\"\"\n\u001b[0;32m    623\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the network!\n",
    "params = {'n_epochs': 100, 'batch_size': 50, 'experiment': 'part1'}\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 5 minutes on CPU or 3 minutes on GPU. Now you are ready to actually modify the functions we used to train our model. Before you move on, make sure to record the accuracy of your network from Part 0, and report it in your write up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modifying the Dataloaders and the Simple Network create_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify the create_datasets function from student_code. You will add random left-right mirroring and normalization to the transformations applied to the training dataset. You will also add normalization to the transformations applied to the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 30\n",
      "Batch 20 / 30\n",
      "Done, mean = \n",
      "[0.45579668]\n",
      "std = \n",
      "[0.23624939]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 60\n",
      "Batch 20 / 60\n",
      "Batch 40 / 60\n",
      "Done, mean = \n",
      "[0.45517009]\n",
      "std = \n",
      "[0.2350788]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify SimpleNet by adding droppout, batch normalization, and additional convolution/maxpool/relu layers. You should achieve an accuracy of at least **50%**. Make sure your network passes this threshold--it is required for full credit on this section!\n",
    "\n",
    "You can also use the following two blocks to determine the stucture of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 5, kernel_size=(9, 9), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(5, 10, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Dropout(p=0.5)\n",
      "  )\n",
      "  (classifier): Conv2d(10, 15, kernel_size=(8, 8), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create the network model\n",
    "model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network output size is  torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "# Use this block to determine the kernel size of the conv2d layer in the classifier\n",
    "# first, set the kernel size of that conv2d layer to 1, and run this block\n",
    "# then, use that size of input to the classifier printed by this block to\n",
    "# go back and update the kernel size of the conv2d layer in the classifier\n",
    "# Finally, run this block again and verify that the network output size is a scalar\n",
    "# Don't forget to re-run the block above every time you update the SimpleNet class!\n",
    "from torch.autograd import Variable\n",
    "data, _ = train_dataset[0]\n",
    "s = data.size()\n",
    "data = Variable(data.view(1, *s))\n",
    "if use_GPU:\n",
    "    data = data.cuda()\n",
    "out = model(data)\n",
    "print('Network output size is ', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. You do not have to modify the custom_part1_trainer in student_code if you use the same loss_function, optimizer, scheduler and parameters (n_epoch, batch_size etc.) as provided in this notebook to hit the required threshold of 50% accuracy. If you changed any of these values, it is important that you modify this function in student_code since we will not be using the notebook you submit to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part1_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part1_trainer = sc.custom_part1_trainer(model)\n",
    "\n",
    "if custom_part1_trainer is None:\n",
    "    # Create the loss function.\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create the optimizer and a learning rate scheduler.\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    # Currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=1)\n",
    "\n",
    "    params = {'n_epochs': 70, 'batch_size': 50, 'experiment': 'part1'}\n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part1_trainer:\n",
    "        loss_function = custom_part1_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part1_trainer:\n",
    "        optimizer = custom_part1_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part1_trainer:\n",
    "        lr_scheduler = custom_part1_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part1_trainer:\n",
    "        params = custom_part1_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train our network! As before, we will start a local server to see the training progress of our network (if you server is already running, you should not start another one). Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Experiment: part1\n",
      "n_epochs: 70\n",
      "resume_optim: True\n",
      "num_workers: 4\n",
      "shuffle: True\n",
      "checkpoint_file: None\n",
      "batch_size: 50\n",
      "do_val: True\n",
      "val_freq: 1\n",
      "print_freq: 100\n",
      "experiment: part1\n",
      "---------------------------------------\n",
      "part1 Epoch 0 / 70\n",
      "train part1: batch 0/29, loss 1.604, top-1 accuracy 40.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.316182\n",
      "val part1: batch 0/59, loss 2.450, top-1 accuracy 22.000, top-5 accuracy 66.000\n",
      "val part1: loss 1.577666\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 1 / 70\n",
      "train part1: batch 0/29, loss 0.973, top-1 accuracy 66.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.287718\n",
      "val part1: batch 0/59, loss 2.171, top-1 accuracy 32.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.608667\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 2 / 70\n",
      "train part1: batch 0/29, loss 1.468, top-1 accuracy 50.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.272832\n",
      "val part1: batch 0/59, loss 2.050, top-1 accuracy 32.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.622098\n",
      "Checkpoint saved\n",
      "part1 Epoch 3 / 70\n",
      "train part1: batch 0/29, loss 1.355, top-1 accuracy 56.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.268921\n",
      "val part1: batch 0/59, loss 2.174, top-1 accuracy 32.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.554873\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 4 / 70\n",
      "train part1: batch 0/29, loss 1.192, top-1 accuracy 64.000, top-5 accuracy 100.000\n",
      "train part1: loss 1.230418\n",
      "val part1: batch 0/59, loss 2.485, top-1 accuracy 16.000, top-5 accuracy 66.000\n",
      "val part1: loss 1.581229\n",
      "Checkpoint saved\n",
      "part1 Epoch 5 / 70\n",
      "train part1: batch 0/29, loss 1.433, top-1 accuracy 52.000, top-5 accuracy 90.000\n",
      "train part1: loss 1.279016\n",
      "val part1: batch 0/59, loss 2.315, top-1 accuracy 26.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.604290\n",
      "Checkpoint saved\n",
      "part1 Epoch 6 / 70\n",
      "train part1: batch 0/29, loss 1.223, top-1 accuracy 60.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.225906\n",
      "val part1: batch 0/59, loss 2.548, top-1 accuracy 20.000, top-5 accuracy 60.000\n",
      "val part1: loss 1.541636\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 7 / 70\n",
      "train part1: batch 0/29, loss 1.362, top-1 accuracy 64.000, top-5 accuracy 90.000\n",
      "train part1: loss 1.248297\n",
      "val part1: batch 0/59, loss 2.297, top-1 accuracy 26.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.582850\n",
      "Checkpoint saved\n",
      "part1 Epoch 8 / 70\n",
      "train part1: batch 0/29, loss 1.179, top-1 accuracy 60.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.201473\n",
      "val part1: batch 0/59, loss 2.213, top-1 accuracy 26.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.565407\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 9 / 70\n",
      "train part1: batch 0/29, loss 1.102, top-1 accuracy 62.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.218324\n",
      "val part1: batch 0/59, loss 2.065, top-1 accuracy 32.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.573182\n",
      "Checkpoint saved\n",
      "part1 Epoch 10 / 70\n",
      "train part1: batch 0/29, loss 1.335, top-1 accuracy 58.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.244077\n",
      "val part1: batch 0/59, loss 2.389, top-1 accuracy 24.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.593407\n",
      "Checkpoint saved\n",
      "part1 Epoch 11 / 70\n",
      "train part1: batch 0/29, loss 1.055, top-1 accuracy 64.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.229888\n",
      "val part1: batch 0/59, loss 2.560, top-1 accuracy 20.000, top-5 accuracy 66.000\n",
      "val part1: loss 1.585870\n",
      "Checkpoint saved\n",
      "part1 Epoch 12 / 70\n",
      "train part1: batch 0/29, loss 1.436, top-1 accuracy 52.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.210410\n",
      "val part1: batch 0/59, loss 1.892, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.544908\n",
      "Checkpoint saved\n",
      "part1 Epoch 13 / 70\n",
      "train part1: batch 0/29, loss 0.872, top-1 accuracy 70.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.155804\n",
      "val part1: batch 0/59, loss 2.440, top-1 accuracy 26.000, top-5 accuracy 66.000\n",
      "val part1: loss 1.548626\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 14 / 70\n",
      "train part1: batch 0/29, loss 0.855, top-1 accuracy 70.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.226996\n",
      "val part1: batch 0/59, loss 2.706, top-1 accuracy 14.000, top-5 accuracy 60.000\n",
      "val part1: loss 1.597986\n",
      "Checkpoint saved\n",
      "part1 Epoch 15 / 70\n",
      "train part1: batch 0/29, loss 1.197, top-1 accuracy 60.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.181090\n",
      "val part1: batch 0/59, loss 2.185, top-1 accuracy 30.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.603314\n",
      "Checkpoint saved\n",
      "part1 Epoch 16 / 70\n",
      "train part1: batch 0/29, loss 0.977, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.136125\n",
      "val part1: batch 0/59, loss 1.979, top-1 accuracy 38.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.579747\n",
      "Checkpoint saved\n",
      "part1 Epoch 17 / 70\n",
      "train part1: batch 0/29, loss 0.879, top-1 accuracy 72.000, top-5 accuracy 100.000\n",
      "train part1: loss 1.218067\n",
      "val part1: batch 0/59, loss 2.223, top-1 accuracy 28.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.574235\n",
      "Checkpoint saved\n",
      "part1 Epoch 18 / 70\n",
      "train part1: batch 0/29, loss 0.873, top-1 accuracy 70.000, top-5 accuracy 100.000\n",
      "train part1: loss 1.141491\n",
      "val part1: batch 0/59, loss 2.241, top-1 accuracy 24.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.576985\n",
      "Checkpoint saved\n",
      "part1 Epoch 19 / 70\n",
      "train part1: batch 0/29, loss 1.289, top-1 accuracy 54.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.157382\n",
      "val part1: batch 0/59, loss 2.501, top-1 accuracy 24.000, top-5 accuracy 64.000\n",
      "val part1: loss 1.566185\n",
      "Checkpoint saved\n",
      "part1 Epoch 20 / 70\n",
      "train part1: batch 0/29, loss 1.008, top-1 accuracy 62.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.166545\n",
      "val part1: batch 0/59, loss 2.209, top-1 accuracy 26.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.638832\n",
      "Checkpoint saved\n",
      "part1 Epoch 21 / 70\n",
      "train part1: batch 0/29, loss 1.329, top-1 accuracy 60.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.144185\n",
      "val part1: batch 0/59, loss 1.981, top-1 accuracy 38.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.580472\n",
      "Checkpoint saved\n",
      "part1 Epoch 22 / 70\n",
      "train part1: batch 0/29, loss 1.192, top-1 accuracy 56.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.115525\n",
      "val part1: batch 0/59, loss 2.159, top-1 accuracy 32.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.618321\n",
      "Checkpoint saved\n",
      "part1 Epoch 23 / 70\n",
      "train part1: batch 0/29, loss 0.961, top-1 accuracy 76.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.090490\n",
      "val part1: batch 0/59, loss 2.096, top-1 accuracy 32.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.525941\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 24 / 70\n",
      "train part1: batch 0/29, loss 0.963, top-1 accuracy 66.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.104538\n",
      "val part1: batch 0/59, loss 2.561, top-1 accuracy 22.000, top-5 accuracy 62.000\n",
      "val part1: loss 1.567428\n",
      "Checkpoint saved\n",
      "part1 Epoch 25 / 70\n",
      "train part1: batch 0/29, loss 0.760, top-1 accuracy 80.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.121934\n",
      "val part1: batch 0/59, loss 2.311, top-1 accuracy 24.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.596461\n",
      "Checkpoint saved\n",
      "part1 Epoch 26 / 70\n",
      "train part1: batch 0/29, loss 1.066, top-1 accuracy 66.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.163083\n",
      "val part1: batch 0/59, loss 1.900, top-1 accuracy 32.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.552680\n",
      "Checkpoint saved\n",
      "part1 Epoch 27 / 70\n",
      "train part1: batch 0/29, loss 0.998, top-1 accuracy 68.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.108432\n",
      "val part1: batch 0/59, loss 1.744, top-1 accuracy 42.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.614221\n",
      "Checkpoint saved\n",
      "part1 Epoch 28 / 70\n",
      "train part1: batch 0/29, loss 1.094, top-1 accuracy 68.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.150360\n",
      "val part1: batch 0/59, loss 1.991, top-1 accuracy 30.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.540756\n",
      "Checkpoint saved\n",
      "part1 Epoch 29 / 70\n",
      "train part1: batch 0/29, loss 0.957, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.074438\n",
      "val part1: batch 0/59, loss 2.271, top-1 accuracy 30.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.587089\n",
      "Checkpoint saved\n",
      "part1 Epoch 30 / 70\n",
      "train part1: batch 0/29, loss 0.982, top-1 accuracy 62.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.138230\n",
      "val part1: batch 0/59, loss 2.726, top-1 accuracy 20.000, top-5 accuracy 56.000\n",
      "val part1: loss 1.576445\n",
      "Checkpoint saved\n",
      "part1 Epoch 31 / 70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train part1: batch 0/29, loss 1.204, top-1 accuracy 58.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.117609\n",
      "val part1: batch 0/59, loss 2.072, top-1 accuracy 30.000, top-5 accuracy 86.000\n",
      "val part1: loss 1.563898\n",
      "Checkpoint saved\n",
      "part1 Epoch 32 / 70\n",
      "train part1: batch 0/29, loss 1.394, top-1 accuracy 58.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.111593\n",
      "val part1: batch 0/59, loss 2.344, top-1 accuracy 26.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.567780\n",
      "Checkpoint saved\n",
      "part1 Epoch 33 / 70\n",
      "train part1: batch 0/29, loss 0.917, top-1 accuracy 66.000, top-5 accuracy 100.000\n",
      "train part1: loss 1.096570\n",
      "val part1: batch 0/59, loss 2.161, top-1 accuracy 30.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.626928\n",
      "Checkpoint saved\n",
      "part1 Epoch 34 / 70\n",
      "train part1: batch 0/29, loss 1.165, top-1 accuracy 56.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.112109\n",
      "val part1: batch 0/59, loss 2.169, top-1 accuracy 30.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.557106\n",
      "Checkpoint saved\n",
      "part1 Epoch 35 / 70\n",
      "train part1: batch 0/29, loss 0.792, top-1 accuracy 74.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.129357\n",
      "val part1: batch 0/59, loss 1.874, top-1 accuracy 38.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.549127\n",
      "Checkpoint saved\n",
      "part1 Epoch 36 / 70\n",
      "train part1: batch 0/29, loss 0.950, top-1 accuracy 66.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.120934\n",
      "val part1: batch 0/59, loss 2.057, top-1 accuracy 26.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.593813\n",
      "Checkpoint saved\n",
      "part1 Epoch 37 / 70\n",
      "train part1: batch 0/29, loss 0.892, top-1 accuracy 76.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.079959\n",
      "val part1: batch 0/59, loss 2.221, top-1 accuracy 26.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.564016\n",
      "Checkpoint saved\n",
      "part1 Epoch 38 / 70\n",
      "train part1: batch 0/29, loss 1.142, top-1 accuracy 62.000, top-5 accuracy 92.000\n",
      "train part1: loss 1.058956\n",
      "val part1: batch 0/59, loss 2.276, top-1 accuracy 26.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.595086\n",
      "Checkpoint saved\n",
      "part1 Epoch 39 / 70\n",
      "train part1: batch 0/29, loss 0.940, top-1 accuracy 76.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.066107\n",
      "val part1: batch 0/59, loss 2.517, top-1 accuracy 22.000, top-5 accuracy 64.000\n",
      "val part1: loss 1.536776\n",
      "Checkpoint saved\n",
      "part1 Epoch 40 / 70\n",
      "train part1: batch 0/29, loss 0.911, top-1 accuracy 62.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.068785\n",
      "val part1: batch 0/59, loss 2.213, top-1 accuracy 26.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.600853\n",
      "Checkpoint saved\n",
      "part1 Epoch 41 / 70\n",
      "train part1: batch 0/29, loss 0.918, top-1 accuracy 74.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.068917\n",
      "val part1: batch 0/59, loss 2.292, top-1 accuracy 28.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.548579\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 42 / 70\n",
      "train part1: batch 0/29, loss 0.995, top-1 accuracy 64.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.043211\n",
      "val part1: batch 0/59, loss 2.297, top-1 accuracy 28.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.675307\n",
      "Checkpoint saved\n",
      "part1 Epoch 43 / 70\n",
      "train part1: batch 0/29, loss 1.500, top-1 accuracy 50.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.037462\n",
      "val part1: batch 0/59, loss 1.762, top-1 accuracy 44.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.541126\n",
      "Checkpoint saved\n",
      "part1 Epoch 44 / 70\n",
      "train part1: batch 0/29, loss 0.889, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.016892\n",
      "val part1: batch 0/59, loss 2.286, top-1 accuracy 32.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.628166\n",
      "Checkpoint saved\n",
      "part1 Epoch 45 / 70\n",
      "train part1: batch 0/29, loss 1.201, top-1 accuracy 60.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.041513\n",
      "val part1: batch 0/59, loss 2.515, top-1 accuracy 24.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.606381\n",
      "Checkpoint saved\n",
      "part1 Epoch 46 / 70\n",
      "train part1: batch 0/29, loss 1.354, top-1 accuracy 60.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.083917\n",
      "val part1: batch 0/59, loss 2.370, top-1 accuracy 32.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.519523\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 47 / 70\n",
      "train part1: batch 0/29, loss 0.877, top-1 accuracy 72.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.099896\n",
      "val part1: batch 0/59, loss 1.876, top-1 accuracy 44.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.551204\n",
      "Checkpoint saved\n",
      "part1 Epoch 48 / 70\n",
      "train part1: batch 0/29, loss 1.002, top-1 accuracy 72.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.059734\n",
      "val part1: batch 0/59, loss 2.130, top-1 accuracy 26.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.530695\n",
      "Checkpoint saved\n",
      "part1 Epoch 49 / 70\n",
      "train part1: batch 0/29, loss 1.015, top-1 accuracy 66.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.023050\n",
      "val part1: batch 0/59, loss 2.003, top-1 accuracy 34.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.598903\n",
      "Checkpoint saved\n",
      "part1 Epoch 50 / 70\n",
      "train part1: batch 0/29, loss 0.786, top-1 accuracy 74.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.008743\n",
      "val part1: batch 0/59, loss 2.106, top-1 accuracy 36.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.573295\n",
      "Checkpoint saved\n",
      "part1 Epoch 51 / 70\n",
      "train part1: batch 0/29, loss 1.293, top-1 accuracy 50.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.038385\n",
      "val part1: batch 0/59, loss 2.218, top-1 accuracy 30.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.533912\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 52 / 70\n",
      "train part1: batch 0/29, loss 1.012, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.968223\n",
      "val part1: batch 0/59, loss 2.152, top-1 accuracy 26.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.550725\n",
      "Checkpoint saved\n",
      "part1 Epoch 53 / 70\n",
      "train part1: batch 0/29, loss 0.880, top-1 accuracy 76.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.001258\n",
      "val part1: batch 0/59, loss 2.417, top-1 accuracy 18.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.582712\n",
      "Checkpoint saved\n",
      "part1 Epoch 54 / 70\n",
      "train part1: batch 0/29, loss 1.239, top-1 accuracy 60.000, top-5 accuracy 96.000\n",
      "train part1: loss 0.975486\n",
      "val part1: batch 0/59, loss 2.283, top-1 accuracy 28.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.522074\n",
      "Checkpoint saved\n",
      "part1 Epoch 55 / 70\n",
      "train part1: batch 0/29, loss 0.950, top-1 accuracy 70.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.024019\n",
      "val part1: batch 0/59, loss 1.898, top-1 accuracy 38.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.525637\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 56 / 70\n",
      "train part1: batch 0/29, loss 0.817, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.026273\n",
      "val part1: batch 0/59, loss 2.338, top-1 accuracy 26.000, top-5 accuracy 72.000\n",
      "val part1: loss 1.549351\n",
      "Checkpoint saved\n",
      "part1 Epoch 57 / 70\n",
      "train part1: batch 0/29, loss 0.704, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "train part1: loss 1.050433\n",
      "val part1: batch 0/59, loss 2.506, top-1 accuracy 28.000, top-5 accuracy 64.000\n",
      "val part1: loss 1.514288\n",
      "Checkpoint saved\n",
      "part1 Epoch 58 / 70\n",
      "train part1: batch 0/29, loss 0.915, top-1 accuracy 74.000, top-5 accuracy 94.000\n",
      "train part1: loss 1.007663\n",
      "val part1: batch 0/59, loss 2.409, top-1 accuracy 28.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.614627\n",
      "Checkpoint saved\n",
      "part1 Epoch 59 / 70\n",
      "train part1: batch 0/29, loss 1.014, top-1 accuracy 66.000, top-5 accuracy 96.000\n",
      "train part1: loss 1.011831\n",
      "val part1: batch 0/59, loss 2.027, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.545604\n",
      "Checkpoint saved\n",
      "part1 Epoch 60 / 70\n",
      "train part1: batch 0/29, loss 0.817, top-1 accuracy 70.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.010345\n",
      "val part1: batch 0/59, loss 2.000, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.576501\n",
      "Checkpoint saved\n",
      "part1 Epoch 61 / 70\n",
      "train part1: batch 0/29, loss 0.729, top-1 accuracy 74.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.999488\n",
      "val part1: batch 0/59, loss 2.327, top-1 accuracy 24.000, top-5 accuracy 68.000\n",
      "val part1: loss 1.540241\n",
      "Checkpoint saved\n",
      "part1 Epoch 62 / 70\n",
      "train part1: batch 0/29, loss 0.796, top-1 accuracy 64.000, top-5 accuracy 98.000\n",
      "train part1: loss 1.007189\n",
      "val part1: batch 0/59, loss 2.087, top-1 accuracy 34.000, top-5 accuracy 78.000\n",
      "val part1: loss 1.616897\n",
      "Checkpoint saved\n",
      "part1 Epoch 63 / 70\n",
      "train part1: batch 0/29, loss 1.049, top-1 accuracy 70.000, top-5 accuracy 94.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train part1: loss 1.025958\n",
      "val part1: batch 0/59, loss 2.580, top-1 accuracy 20.000, top-5 accuracy 58.000\n",
      "val part1: loss 1.623045\n",
      "Checkpoint saved\n",
      "part1 Epoch 64 / 70\n",
      "train part1: batch 0/29, loss 0.868, top-1 accuracy 72.000, top-5 accuracy 96.000\n",
      "train part1: loss 0.996509\n",
      "val part1: batch 0/59, loss 2.710, top-1 accuracy 22.000, top-5 accuracy 64.000\n",
      "val part1: loss 1.595782\n",
      "Checkpoint saved\n",
      "part1 Epoch 65 / 70\n",
      "train part1: batch 0/29, loss 0.957, top-1 accuracy 66.000, top-5 accuracy 96.000\n",
      "train part1: loss 0.998652\n",
      "val part1: batch 0/59, loss 2.565, top-1 accuracy 16.000, top-5 accuracy 70.000\n",
      "val part1: loss 1.537408\n",
      "Checkpoint saved\n",
      "part1 Epoch 66 / 70\n",
      "train part1: batch 0/29, loss 0.863, top-1 accuracy 76.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.960142\n",
      "val part1: batch 0/59, loss 2.386, top-1 accuracy 24.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.571341\n",
      "Checkpoint saved\n",
      "part1 Epoch 67 / 70\n",
      "train part1: batch 0/29, loss 1.048, top-1 accuracy 62.000, top-5 accuracy 96.000\n",
      "train part1: loss 0.986436\n",
      "val part1: batch 0/59, loss 2.239, top-1 accuracy 28.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.502871\n",
      "Checkpoint saved\n",
      "part1 Epoch 68 / 70\n",
      "train part1: batch 0/29, loss 0.871, top-1 accuracy 68.000, top-5 accuracy 96.000\n",
      "train part1: loss 0.955852\n",
      "val part1: batch 0/59, loss 1.987, top-1 accuracy 30.000, top-5 accuracy 86.000\n",
      "val part1: loss 1.554071\n",
      "Checkpoint saved\n",
      "part1 Epoch 69 / 70\n",
      "train part1: batch 0/29, loss 0.793, top-1 accuracy 76.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.993873\n",
      "val part1: batch 0/59, loss 2.442, top-1 accuracy 24.000, top-5 accuracy 74.000\n",
      "val part1: loss 1.541226\n",
      "Checkpoint saved\n",
      "Best top-1 Accuracy = 52.563\n"
     ]
    }
   ],
   "source": [
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you get at least 50% accuracy in this section! If you tried different settings than the ones provided to get 50%, you should modify custom_part1_trainer in student code to return a dictionary with your changed settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Fine-Tuning a Pre-Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network from scratch takes a lof of time. Instead of training from scratch, we can take a pre-trained model and fine tune it for our purposes. This is the goal of Part 2--you will train a pre-trained network, and achieve at least 80% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "input_size = (224, 224)\n",
    "RGB = True\n",
    "base_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9\n",
    "backprop_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 30\n",
      "Batch 20 / 30\n",
      "Done, mean = \n",
      "[0.45611589 0.45611589 0.45611589]\n",
      "std = \n",
      "[0.24786406 0.24786406 0.24786406]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 60\n",
      "Batch 20 / 60\n",
      "Batch 40 / 60\n",
      "Done, mean = \n",
      "[0.45549639 0.45549639 0.45549639]\n",
      "std = \n",
      "[0.24698076 0.24698076 0.24698076]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block loads a pretrained AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create the network model.\n",
    "model = alexnet(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you modify create_part2_model from student code in order to fine-tune AlexNet. As you can see in the docs (https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) and in the model printout above, AlexNet has 2 parts: 'features', which constists of conv layers that extract feature maps from the image, and 'classifier' which consists of FC layers that classify the features. We want to replace the last Linear layer in model.classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = sc.create_part2_model(model, num_classes)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. Just as with part 1, if you modify any of the setttings to hit the required accuracy, you must modify custom_part2_trainer function to return a dictionary containing your changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part2_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part2_trainer = sc.custom_part2_trainer(model)\n",
    "\n",
    "if custom_part2_trainer is None:\n",
    "    # Create the loss function\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Since we do not want to optimize the whole network, we must extract a list of parameters of interest that will be\n",
    "    # optimized by the optimizer.\n",
    "    params_to_optimize = []\n",
    "\n",
    "    # List of modules in the network\n",
    "    mods = list(model.features.children()) + list(model.classifier.children())\n",
    "\n",
    "    # Extract parameters from the last `backprop_depth` modules in the network and collect them in\n",
    "    # the params_to_optimize list.\n",
    "    for m in mods[::-1][:backprop_depth]:\n",
    "        params_to_optimize.extend(list(m.parameters()))\n",
    "\n",
    "    # Construct the optimizer    \n",
    "    optimizer = optim.SGD(params=params_to_optimize, lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    # Create a scheduler, currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 20, gamma = 1)\n",
    "    \n",
    "    params = {'n_epochs': 4, 'batch_size': 10, 'experiment': 'part2'} \n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part2_trainer:\n",
    "        loss_function = custom_part2_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part2_trainer:\n",
    "        optimizer = custom_part2_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part2_trainer:\n",
    "        lr_scheduler = custom_part2_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part2_trainer:\n",
    "        params = custom_part2_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fine tune our network! Just like before, we will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 2 on the top left bar where is says Environment (only select Part 2, do not check main or Part 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Experiment: part2\n",
      "n_epochs: 4\n",
      "resume_optim: True\n",
      "num_workers: 4\n",
      "shuffle: True\n",
      "checkpoint_file: None\n",
      "batch_size: 10\n",
      "do_val: True\n",
      "val_freq: 1\n",
      "print_freq: 100\n",
      "experiment: part2\n",
      "---------------------------------------\n",
      "part2 Epoch 0 / 4\n",
      "train part2: batch 0/149, loss 2.378, top-1 accuracy 30.000, top-5 accuracy 80.000\n",
      "train part2: batch 100/149, loss 1.203, top-1 accuracy 90.000, top-5 accuracy 90.000\n",
      "train part2: loss 1.998524\n",
      "val part2: batch 0/298, loss 0.980, top-1 accuracy 40.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.290, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.293, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: loss 1.059358\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part2 Epoch 1 / 4\n",
      "train part2: batch 0/149, loss 1.697, top-1 accuracy 50.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 1.528, top-1 accuracy 50.000, top-5 accuracy 90.000\n",
      "train part2: loss 1.175488\n",
      "val part2: batch 0/298, loss 1.741, top-1 accuracy 20.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.260, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.218, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.665452\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part2 Epoch 2 / 4\n",
      "train part2: batch 0/149, loss 0.275, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 0.848, top-1 accuracy 60.000, top-5 accuracy 90.000\n",
      "train part2: loss 0.983571\n",
      "val part2: batch 0/298, loss 2.183, top-1 accuracy 20.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.419, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.361, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.676417\n",
      "Checkpoint saved\n",
      "part2 Epoch 3 / 4\n",
      "train part2: batch 0/149, loss 0.559, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 1.816, top-1 accuracy 50.000, top-5 accuracy 90.000\n",
      "train part2: loss 0.950080\n",
      "val part2: batch 0/298, loss 1.355, top-1 accuracy 30.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.497, top-1 accuracy 70.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.924, top-1 accuracy 60.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.571132\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "Best top-1 Accuracy = 80.000\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 10 minutes on CPU or 30 seconds on GPU. You should hit 80% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
